{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109de5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from time import time, sleep\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from conf import conf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='classifierdl_use_emotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34daba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumerConf = {'bootstrap.servers': conf.BOOTSTRAP_SERVER,\n",
    "        'group.id': \"AfekaFinalProj\",\n",
    "        'auto.offset.reset': 'smallest'}\n",
    "\n",
    "consumer = Consumer(consumerConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "\n",
    "sentimentdl = ClassifierDLModel.pretrained(name=MODEL_NAME)\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipelineModel = nlpPipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672e6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "running = True\n",
    "\n",
    "def basic_consume_loop(consumer, topics):\n",
    "    try:        \n",
    "        consumer.subscribe(topics)\n",
    "        \n",
    "        start_time = time()\n",
    "        seconds = 2\n",
    "\n",
    "        index = 1\n",
    "        batch = []\n",
    "        \n",
    "        while running:         \n",
    "            msg = consumer.poll(timeout=3.0)            \n",
    "            \n",
    "            current_time = time()\n",
    "            elapsed_time = current_time - start_time\n",
    "    \n",
    "            # create a new file\n",
    "            if elapsed_time > seconds:               \n",
    "                \n",
    "                if len(batch) > 0:      \n",
    "\n",
    "                    df = spark.createDataFrame(pd.DataFrame({\"text\":batch}))\n",
    "                    result = pipelineModel.transform(df) \n",
    "                    result.select(F.explode(F.arrays_zip('document.result', 'sentiment.result')).alias(\"cols\")).select(F.expr(\"cols['0']\").alias(\"document\"),F.expr(\"cols['1']\").alias(\"sentiment\")).show(truncate=False)\n",
    "                    print('----------------')\n",
    "\n",
    "                    file_name = 'AMAZON_FASHION_' + str(index) + '.txt'\n",
    "                    print(\"File {} has been created. File size: {} lines\".format(file_name, len(batch)))\n",
    "\n",
    "                    # Create the folder if it does not exists\n",
    "                    path = 'AMAZON_FASHION'                    \n",
    "                    os.makedirs(path, exist_ok=True) \n",
    "                    \n",
    "                    complete_name = os.path.join('AMAZON_FASHION', file_name)\n",
    "                    with open(complete_name, 'w') as f:\n",
    "                        for item in batch:\n",
    "                            f.write(\"%s\\n\" % item)\n",
    "                    \n",
    "                    batch = []\n",
    "                    index +=1\n",
    "                \n",
    "                start_time = time()  \n",
    "                \n",
    "            if msg is None: continue\n",
    "\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    # End of partition event\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                     (msg.topic(), msg.partition(), msg.offset()))\n",
    "                elif msg.error():\n",
    "                    raise KafkaException(msg.error())\n",
    "            else:\n",
    "                batch.append(msg.value().decode('utf-8'))\n",
    "                \n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        consumer.close()\n",
    "\n",
    "def shutdown():\n",
    "    running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02831eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File AMAZON_FASHION_1.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_2.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_3.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_4.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_5.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_6.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_7.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_8.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_9.txt has been created. File size: 100 lines\n",
      "File AMAZON_FASHION_10.txt has been created. File size: 100 lines\n"
     ]
    }
   ],
   "source": [
    "basic_consume_loop(consumer, [conf.KAFKA_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef717dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
